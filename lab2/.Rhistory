data[,1:5] <- scale(data[,1:5])
data1 <- data
for (i in 1:4) {
mdl <- lm(Infant.Mortality ~ poly(Fertility, degree=i), data = data)
data1[,6+i] <- predict(mdl,data)
}
ggplot(data1)+
geom_point(aes(x=Fertility,y=Infant.Mortality))+
geom_line(data=data1, aes(x=Fertility,y=V7))+
geom_line(data=data1, aes(x=Fertility,y=V8))+
geom_line(data=data1, aes(x=Fertility,y=V9))+
geom_line(data=data1, aes(x=Fertility,y=V10))
swiss
rm(list=ls())
data <- swiss
data[,1:5] <- scale(data[,1:5])
data1 <- data
for (i in 1:6) {
mdl <- lm(Examination ~ poly(Education, degree=i), data = data)
data1[,6+i] <- predict(mdl,data)
}
ggplot(data1)+
geom_point(aes(x=Education,y=Examination))+
geom_line(data=data1, aes(x=Education,y=V7))+
geom_line(data=data1, aes(x=Education,y=V8))+
geom_line(data=data1, aes(x=Education,y=V9))+
geom_line(data=data1, aes(x=Education,y=V10))+
geom_line(data=data1, aes(x=Education,y=V11),color="red")+
geom_line(data=data1, aes(x=Education,y=V12))
rm(list=ls())
data <- swiss
data[,1:5] <- scale(data[,1:5])
data1 <- data
for (i in 1:3) {
mdl <- lm(Examination ~ poly(Education, degree=i), data = data)
data1[,6+i] <- predict(mdl,data)
}
ggplot(data1)+
geom_point(aes(x=Education,y=Examination))+
geom_line(data=data1, aes(x=Education,y=V7))+
geom_line(data=data1, aes(x=Education,y=V8))+
geom_line(data=data1, aes(x=Education,y=V9))
rm(list=ls())
data <- swiss
data[,1:5] <- scale(data[,1:5])
data1 <- data
for (i in 1:3) {
mdl <- lm(Examination ~ poly(Education, degree=i), data = data)
data1[,6+i] <- predict(mdl,data)
}
ggplot(data1)+
geom_point(aes(x=Education,y=Examination))+
geom_line(data=data1, aes(x=Education,y=V7),color="red")+
geom_line(data=data1, aes(x=Education,y=V8)color="blue")+
geom_line(data=data1, aes(x=Education,y=V9)color="pink")
rm(list=ls())
data <- swiss
data[,1:5] <- scale(data[,1:5])
data1 <- data
for (i in 1:3) {
mdl <- lm(Examination ~ poly(Education, degree=i), data = data)
data1[,6+i] <- predict(mdl,data)
}
ggplot(data1)+
geom_point(aes(x=Education,y=Examination))+
geom_line(data=data1, aes(x=Education,y=V7),color="red")+
geom_line(data=data1, aes(x=Education,y=V8),color="blue")+
geom_line(data=data1, aes(x=Education,y=V9),color="pink")
ggplot(data1)+
geom_point(aes(x=Education,y=Examination))+
geom_line(data=data1, aes(x=Education,y=V7),color="red")+
geom_line(data=data1, aes(x=Education,y=V8),color="blue")+
geom_line(data=data1, aes(x=Education,y=V9),color="green")
rm(list=ls())
data <- swiss
data[,1:5] <- scale(data[,1:5])
mdl <- lm(Examination ~, data = data)
rm(list=ls())
data <- swiss
data[,1:5] <- scale(data[,1:5])
mdl <- lm(Infant.Mortality~., data = data)
mdl_AIC <- stepAIC(md, direction = 'both',trace = FALSE )
cat("Number of remaining variables:",length(mdl_AIC$coefficients),"\n")
data[,1:5] <- scale(data[,1:5])
mdl <- lm(Infant.Mortality~., data = data)
require(MASS)
mdl_AIC <- stepAIC(md, direction = 'both',trace = FALSE )
mdl_AIC <- stepAIC(mdl, direction = 'both',trace = FALSE )
cat("Number of remaining variables:",length(mdl_AIC$coefficients),"\n")
summary(mdl)
mdl$coefficients
mdl_AIC$coefficients
rm(list=ls())
data <- swiss
data[,1:5] <- scale(data[,1:5])
mdl <- lm(Infant.Mortality~., data = data)
require(MASS)
mdl_AIC <- stepAIC(mdl, direction = 'both',trace = FALSE )
cat("Number of remaining variables:",length(mdl_AIC$coefficients),"\n")
pred <- predict(mdl_AIC, data)
eAIC <- mean((pred-data$Infant.Mortality)^2 )
rm(list=ls())
data <- swiss
data[,1:5] <- scale(data[,1:5])
mdl <- lm(Infant.Mortality~., data = data)
require(MASS)
mdl_AIC <- stepAIC(mdl, direction = 'both',trace = FALSE )
cat("Number of remaining variables:",length(mdl_AIC$coefficients),"\n")
pred <- predict(mdl_AIC, data)
mean((pred-data$Infant.Mortality)**2 )
require(glmnet)
X <- as.matrix(data[,1:5])
Y <- as.matrix(data[,6])
md_RR <- glmnet(X, Y, alpha = 0, family = "gaussian")
plot(md_RR, xvar="lambda", label=TRUE)
require(glmnet)
X <- as.matrix(data[,1:5])
Y <- as.matrix(data[,6])
mdl_RR <- glmnet(X, Y, alpha = 0, family = "gaussian")
plot(mdl_RR, xvar="lambda", label=TRUE)
pred <- predict(mdl_RR, data)
mean((pred-data$Infant.Mortality)**2 )
require(glmnet)
X <- as.matrix(data[,1:5])
Y <- as.matrix(data[,6])
mdl_RR <- glmnet(X, Y, alpha = 0, family = "gaussian")
plot(mdl_RR, xvar="lambda", label=TRUE)
pred <- predict(mdl_RR, data, s="lambda.min")
pred <- predict(mdl_RR, X, s="lambda.min")
mdl_LASSO <- glmnet(X, Y, alpha = 1, family = "gaussian")
plot(mdl_LASSO, xvar="lambda", label=TRUE)
cvfit=cv.glmnet(X, Y,family = "gaussian", alpha = 1, type.measure = "mse",
lambda = seq(0,0.01,0.000001))
p <- predict(cvfit, newx=X, s="lambda.1se")
plot(cvfit, ylab="cv(MSE)")
cvfit=cv.glmnet(X, Y,family = "gaussian", alpha = 1, type.measure = "mse")
plot(cvfit, ylab="cv(MSE)")
cvfit=cv.glmnet(X, Y,family = "gaussian", alpha = 1, type.measure = "mse")
p <- predict(cvfit, newx=X, s="lambda.1se")
eLASSO <- mean((p-data$Infant.Mortality)^2 )
cvfit=cv.glmnet(X, Y,family = "gaussian", alpha = 1, type.measure = "mse")
p <- predict(cvfit, newx=X, s="lambda.1se")
mean((p-data$Infant.Mortality)^2 )
cvfit=cv.glmnet(X, Y,family = "gaussian", alpha = 1, type.measure = "mse")
c(cvfit$lambda.min,cvfit$lambda.se)
cvfit$lambda.se
c(cvfit$lambda.min,cvfit$lambda.1se)
p <- predict(cvfit, newx=X, s="lambda.1se")
mean((p-data$Infant.Mortality)^2 )
co <- coef(cvfit,s=cvfit$lambda.1se)
length(co@x)
plot(cvfit, ylab="cv(MSE)")
setwd("C:\\Users\\fengy\Desktop\\732A99 Machine Learning\\lab2")
setwd("C:\\Users\\fengy\\Desktop\\732A99 Machine Learning\\lab2")
data <- swiss
library(tree)
library(rpart)
t21=tree(Infant.Mortality~., data=data,
split = c("deviance","gini"))
mdl=tree(Infant.Mortality~., data=data,
split = c("deviance","gini"))
plot(mdl)
text(mdl, pretty=0)
rm(list=ls())
data <- swiss
id <- 1:30
train <- data[id,]
test <- data[-id,]
library(tree)
library(rpart)
mdl=tree(Infant.Mortality~., data=train,
split = c("deviance","gini"))
plot(mdl)
text(mdl, pretty=0)
pred <- predict(mdl, test, type = "class")
mean(p211!=trainInfant.Mortality)
rm(list=ls())
data <- swiss
id <- 1:30
train <- data[id,]
test <- data[-id,]
library(tree)
library(rpart)
mdl=tree(Infant.Mortality~., data=train,
split = c("deviance","gini"))
plot(mdl)
text(mdl, pretty=0)
pred <- predict(mdl, test, type = "class")
rm(list=ls())
data <- spam
data(spam)
data <- spam
id <- 1:2000
train <- data[id,]
test <- data[-id,]
library(tree)
library(rpart)
mdl=tree(type~., data=train,
split = c("deviance","gini"))
plot(mdl)
text(mdl, pretty=0)
pred <- predict(mdl, test, type = "class")
mean(p211!=trainInfant.Mortality)
mean(pred!=trainInfant.Mortality)
mean(pred!=test$type)
ptree <- prune.tree(mdl,best=4)
plot(ptree)
text(ptree, pretty=0)
nodes <- as.numeric(rownames(ptree$frame))
max(rpart:::tree.depth(nodes))
deviance(ptree)
class(ptree)
rm(list=ls())
data(spam)
data <- spam
id <- 1:2000
train <- data[id,]
test <- data[-id,]
library(tree)
library(rpart)
mdl=tree(type~., data=train,
split = c("deviance","gini"))
plot(mdl)
text(mdl, pretty=0)
pred <- predict(mdl, test, type = "class")
mean(pred!=test$type)
# [1] 0.3440984
ptree <- prune.tree(mdl,best=4)
class(ptree)
class(mdl)
ptree_test <- predict(ptree,newdata=test,type="tree")
deviance(ptree_test)
deviance(ptree)
deviance(tree)
deviance(mdl)
deviance(ptree)
deviance(ptree_test)
ptree_test <- predict(mdl,newdata=test,type="tree")
deviance(ptree_test)
library(e1071)
b=naiveBayes(type~., data=train)
nb=naiveBayes(type~., data=train)
library(e1071)
nb=naiveBayes(good_bad~., data=train)
nb=naiveBayes(type~., data=train)
pred <- predict(b, newdata=test)
table(test$type,pred,dnn = c("Labels","prediction"))
pred <- predict(nb, newdata=test)
table(test$type,pred,dnn = c("Labels","prediction"))
mean(pred!=test$type)
p51 <- predict(ptree, test, type="vector")
p52 <- predict(nb, test, type="raw")
i=1
pp51 <- factor(p51[,2]>dt5[i,1],labels = c("bad","good"),levels = c("FALSE","TRUE"))
dt5 <- as.data.frame(matrix(nrow = 19,ncol = 5))
dt5[,1] <- seq(.05,0.95,0.05)
for (i in 1:19) {
p51 <- predict(ptree, test, type="vector")
p52 <- predict(nb, test, type="raw")
pp51 <- factor(p51[,2]>dt5[i,1],labels = c("bad","good"),levels = c("FALSE","TRUE"))
pp52 <- factor(p52[,2]>dt5[i,1],labels = c("bad","good"),levels = c("FALSE","TRUE"))
mat_51 <- table(test$good_bad,pp51,dnn = c("Labels","prediction"))
mat_52 <- table(test$good_bad,pp52,dnn = c("Labels","prediction"))
temp <-c(mat_51[2,2]/sum(mat_51[2,]), mat_51[1,2]/sum(mat_51[1,]),
mat_52[2,2]/sum(mat_52[2,]), mat_52[1,2]/sum(mat_52[1,]))
dt5[i,2:5] <- temp
}
names(dt5) <- c("pi","TPR", "FPR","TPR", "FPR")
dt55 <- rbind(dt5[,2:3],dt5[,4:5])
dt55$group <- c(rep("optimal tree",19),rep("naive bayes",19))
library(ggplot2)
ggplot(dt55, aes(x=FPR,y=TPR,color=group))+
geom_line()+
labs(title="ROC curves")
rm(list=ls())
data(spam)
data <- spam
id <- 1:2000
train <- data[id,]
test <- data[-id,]
library(e1071)
nb=naiveBayes(type~., data=train)
pred <- predict(nb, newdata=test)
mean(pred!=test$type)
rm(list=ls())
data(spam)
data <- spam
id <- sample(1:4601,2000)
train <- data[id,]
test <- data[-id,]
library(e1071)
nb=naiveBayes(type~., data=train)
pred <- predict(nb, newdata=test)
mean(pred!=test$type)
table(test$type,pred,dnn = c("Labels","prediction"))
library(tree)
library(rpart)
mdl=tree(type~., data=train,
split = c("deviance","gini"))
plot(mdl)
text(mdl, pretty=0)
pred <- predict(mdl, test, type = "class")
mean(pred!=test$type)
deviance(mdl)
tree_test <- predict(mdl,newdata=test,type="tree")
deviance(tree_test)
ptree <- prune.tree(mdl,best=4)
plot(ptree)
text(ptree, pretty=0)
deviance(ptree)
nodes <- as.numeric(rownames(ptree$frame))
max(rpart:::tree.depth(nodes))
tree_test <- predict(mdl,newdata=test,type="tree")
deviance(tree_test)
tree_test <- predict(ptree,newdata=test,type="tree")
deviance(tree_test)
library(e1071)
nb=naiveBayes(type~., data=train)
pred <- predict(nb, newdata=test)
mean(pred!=test$type)
dt5 <- as.data.frame(matrix(nrow = 19,ncol = 5))
dt5[,1] <- seq(.05,0.95,0.05)
for (i in 1:19) {
p51 <- predict(ptree, test, type="vector")
p52 <- predict(nb, test, type="raw")
pp51 <- factor(p51[,2]>dt5[i,1],labels = c("bad","good"),levels = c("FALSE","TRUE"))
pp52 <- factor(p52[,2]>dt5[i,1],labels = c("bad","good"),levels = c("FALSE","TRUE"))
mat_51 <- table(test$good_bad,pp51,dnn = c("Labels","prediction"))
mat_52 <- table(test$good_bad,pp52,dnn = c("Labels","prediction"))
temp <-c(mat_51[2,2]/sum(mat_51[2,]), mat_51[1,2]/sum(mat_51[1,]),
mat_52[2,2]/sum(mat_52[2,]), mat_52[1,2]/sum(mat_52[1,]))
dt5[i,2:5] <- temp
}
names(dt5) <- c("pi","TPR", "FPR","TPR", "FPR")
dt55 <- rbind(dt5[,2:3],dt5[,4:5])
dt55$group <- c(rep("optimal tree",19),rep("naive bayes",19))
library(ggplot2)
ggplot(dt55, aes(x=FPR,y=TPR,color=group))+
geom_line()+
labs(title="ROC curves")
dt5 <- as.data.frame(matrix(nrow = 19,ncol = 5))
dt5[,1] <- seq(.05,0.95,0.05)
dt5
i=1
p51 <- predict(ptree, test, type="vector")
p52 <- predict(nb, test, type="raw")
pp51 <- factor(p51[,2]>dt5[i,1],labels = c("bad","good"),levels = c("FALSE","TRUE"))
pp52 <- factor(p52[,2]>dt5[i,1],labels = c("bad","good"),levels = c("FALSE","TRUE"))
pp51
pp52
pp51
p51 <- predict(ptree, test, type="vector")
p52 <- predict(nb, test, type="raw")
p51
p51 <- predict(ptree, test, type="vector")
p52 <- predict(nb, test, type="raw")
pp51 <- factor(p51[,2]>dt5[i,1],labels = c("nonspam","spam"),levels = c("FALSE","TRUE"))
pp52 <- factor(p52[,2]>dt5[i,1],labels = c("nonspam","spam"),levels = c("FALSE","TRUE"))
table(test$type,pp51,dnn = c("Labels","prediction"))
dt5 <- as.data.frame(matrix(nrow = 19,ncol = 5))
dt5[,1] <- seq(.05,0.95,0.05)
for (i in 1:19) {
p51 <- predict(ptree, test, type="vector")
p52 <- predict(nb, test, type="raw")
pp51 <- factor(p51[,2]>dt5[i,1],labels = c("nonspam","spam"),levels = c("FALSE","TRUE"))
pp52 <- factor(p52[,2]>dt5[i,1],labels = c("nonspam","spam"),levels = c("FALSE","TRUE"))
mat_51 <- table(test$type,pp51,dnn = c("Labels","prediction"))
mat_52 <- table(test$type,pp52,dnn = c("Labels","prediction"))
temp <-c(mat_51[2,2]/sum(mat_51[2,]), mat_51[1,2]/sum(mat_51[1,]),
mat_52[2,2]/sum(mat_52[2,]), mat_52[1,2]/sum(mat_52[1,]))
dt5[i,2:5] <- temp
}
dt5
names(dt5) <- c("pi","TPR", "FPR","TPR", "FPR")
dt55 <- rbind(dt5[,2:3],dt5[,4:5])
dt55$group <- c(rep("optimal tree",19),rep("naive bayes",19))
library(ggplot2)
ggplot(dt55, aes(x=FPR,y=TPR,color=group))+
geom_line()+
labs(title="ROC curves")
dt5
test <- test[1:500,]
dt5 <- as.data.frame(matrix(nrow = 19,ncol = 5))
dt5[,1] <- seq(.05,0.95,0.05)
test <- test[1:500,]
dt5 <- as.data.frame(matrix(nrow = 19,ncol = 5))
dt5[,1] <- seq(.05,0.95,0.1)
nb=naiveBayes(type~., data=train)
rm(list=ls())
data(spam)
data <- spam
id <- sample(1:4601,2000)
train <- data[id,]
test <- data[-id,]
library(tree)
library(rpart)
mdl=tree(type~., data=train,
split = c("deviance","gini"))
mdl=tree(type~., data=train,
split = c("deviance","gini"))
ptree <- prune.tree(mdl,best=4)
test <- test[1:500,]
dt5 <- as.data.frame(matrix(nrow = 19,ncol = 5))
dt5[,1] <- seq(.05,0.95,0.1)
dt5 <- as.data.frame(matrix(nrow = 10,ncol = 5))
dt5[,1] <- seq(.05,0.95,0.1)
for (i in 1:10) {
p51 <- predict(ptree, test, type="vector")
p52 <- predict(nb, test, type="raw")
pp51 <- factor(p51[,2]>dt5[i,1],labels = c("nonspam","spam"),levels = c("FALSE","TRUE"))
pp52 <- factor(p52[,2]>dt5[i,1],labels = c("nonspam","spam"),levels = c("FALSE","TRUE"))
mat_51 <- table(test$type,pp51,dnn = c("Labels","prediction"))
mat_52 <- table(test$type,pp52,dnn = c("Labels","prediction"))
temp <-c(mat_51[2,2]/sum(mat_51[2,]), mat_51[1,2]/sum(mat_51[1,]),
mat_52[2,2]/sum(mat_52[2,]), mat_52[1,2]/sum(mat_52[1,]))
dt5[i,2:5] <- temp
}
nb=naiveBayes(type~., data=train)
pred <- predict(nb, newdata=test)
mean(pred!=test$type)
test <- test[1:500,]
dt5 <- as.data.frame(matrix(nrow = 10,ncol = 5))
dt5[,1] <- seq(.05,0.95,0.1)
for (i in 1:10) {
p51 <- predict(ptree, test, type="vector")
p52 <- predict(nb, test, type="raw")
pp51 <- factor(p51[,2]>dt5[i,1],labels = c("nonspam","spam"),levels = c("FALSE","TRUE"))
pp52 <- factor(p52[,2]>dt5[i,1],labels = c("nonspam","spam"),levels = c("FALSE","TRUE"))
mat_51 <- table(test$type,pp51,dnn = c("Labels","prediction"))
mat_52 <- table(test$type,pp52,dnn = c("Labels","prediction"))
temp <-c(mat_51[2,2]/sum(mat_51[2,]), mat_51[1,2]/sum(mat_51[1,]),
mat_52[2,2]/sum(mat_52[2,]), mat_52[1,2]/sum(mat_52[1,]))
dt5[i,2:5] <- temp
}
names(dt5) <- c("pi","TPR", "FPR","TPR", "FPR")
dt55 <- rbind(dt5[,2:3],dt5[,4:5])
dt55$group <- c(rep("optimal tree",19),rep("naive bayes",19))
library(ggplot2)
dt55$group <- c(rep("optimal tree",10),rep("naive bayes",10))
library(ggplot2)
ggplot(dt55, aes(x=FPR,y=TPR,color=group))+
geom_line()+
labs(title="ROC curves")
dt5
test <- test[1:500,]
dt5 <- as.data.frame(matrix(nrow = 10,ncol = 5))
dt5[,1] <- seq(.05,0.95,0.1)
i=1
p51 <- predict(ptree, test, type="vector")
p52 <- predict(nb, test, type="raw")
pp51 <- factor(p51[,2]>dt5[i,1],labels = c("nonspam","spam"),levels = c("FALSE","TRUE"))
pp52 <- factor(p52[,2]>dt5[i,1],labels = c("nonspam","spam"),levels = c("FALSE","TRUE"))
mat_51 <- table(test$type,pp51,dnn = c("Labels","prediction"))
mat_52 <- table(test$type,pp52,dnn = c("Labels","prediction"))
mat_51
mat_52
temp <-c(mat_51[2,2]/sum(mat_51[2,]), mat_51[1,2]/sum(mat_51[1,]),
mat_52[2,2]/sum(mat_52[2,]), mat_52[1,2]/sum(mat_52[1,]))
temp
rm(list=ls())
data(spam)
data <- spam
id <- sample(1:4601,2000)
train <- data[id,]
test <- data[-id,]
library(e1071)
nb=naiveBayes(type~., data=train)
pred <- predict(nb, newdata=test)    # type="raw" 显示近01两者的概率
mean(pred!=test$type)
plot(nb)
#4.1#####################################################
mydata <- read.csv2("NIRspectra.csv")
data1=mydata
data1[,"Viscosity"]=c() # remove targets
res=prcomp(data1,scale=FALSE)
lambda=res$sdev^2 #eigenvalues
screeplot(res) # plots the variances against the number of the principal component
?screeplot
proportion <- sprintf("%2.3f",lambda/sum(lambda)*100) # proportions of variations
# "%2.3f" is precision
sum(as.numeric(proportion)[1:2]) # 99.595>99
library(ggplot2)
dt1 <- as.data.frame(res$x[,1:2])
ggplot(data = dt1, aes(x=PC1,y=PC2))+
geom_point()
#4.1#####################################################
mydata <- read.csv2("NIRspectra.csv")
data1=mydata
data1[,"Viscosity"]=c() # remove targets
res=prcomp(data1,scale=TRUE)
lambda=res$sdev^2 #eigenvalues
screeplot(res) # plots the variances against the number of the principal component
proportion <- sprintf("%2.3f",lambda/sum(lambda)*100) # proportions of variations
#4.1#####################################################
mydata <- read.csv2("NIRspectra.csv")
data1=mydata
data1[,"Viscosity"]=c() # remove targets
res=prcomp(data1,scale=FALSE)
lambda=res$sdev^2 #eigenvalues
# lambda  # 126 lambda
screeplot(res) # plots the variances against the number of the principal component
proportion <- sprintf("%2.3f",lambda/sum(lambda)*100) # proportions of variations
# "%2.3f" is precision
sum(as.numeric(proportion)[1:2]) # 99.595>99
library(ggplot2)
dt1 <- as.data.frame(res$x[,1:2])
ggplot(data = dt1, aes(x=PC1,y=PC2))+
geom_point()
res
res$center
?prompt
?prcomp
